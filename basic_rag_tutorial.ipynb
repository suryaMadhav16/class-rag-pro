{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23543280",
   "metadata": {},
   "source": [
    "# Building a Basic RAG System: A Step-by-Step Guide\n",
    "\n",
    "![RAG System Architecture](https://miro.medium.com/v2/resize:fit:1400/1*7i-6SZo3C65T3mrqUxpgZQ.png)\n",
    "\n",
    "## Introduction to RAG (Retrieval Augmented Generation)\n",
    "\n",
    "Retrieval Augmented Generation (RAG) combines the power of retrieval-based systems with generative AI to create more accurate, informative, and contextually relevant responses. Instead of relying solely on the knowledge encoded in a language model's parameters, RAG systems first retrieve relevant information from a knowledge base and then use that information to generate responses.\n",
    "\n",
    "Key components of a RAG system include:\n",
    "\n",
    "1. **Document Collection**: A corpus of documents containing domain-specific information\n",
    "2. **Document Processing**: Converting raw documents into a format suitable for embedding\n",
    "3. **Chunking**: Breaking documents into smaller, manageable pieces\n",
    "4. **Embedding**: Converting text chunks into vector representations\n",
    "5. **Vector Storage**: Storing embeddings in a vector database for efficient retrieval\n",
    "6. **Retrieval**: Finding the most relevant chunks for a given query\n",
    "7. **Augmentation**: Combining the query with retrieved context\n",
    "8. **Generation**: Using an LLM to generate a response based on the augmented prompt\n",
    "\n",
    "In this tutorial, we'll build a complete RAG system from scratch and see it in action!\n",
    "\n",
    "## Setting Up the Environment\n",
    "\n",
    "First, let's install the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e415017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (3.3.1)\n",
      "Requirement already satisfied: chromadb in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (0.6.3)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (1.59.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from sentence-transformers) (4.48.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from sentence-transformers) (0.27.1)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: build>=1.0.3 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (2.8.2)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (0.115.6)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (1.23.5)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (3.21.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (1.31.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (1.31.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (0.52b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (1.31.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (0.21.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (0.12.5)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (32.0.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (3.10.14)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (0.27.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: packaging>=19.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (0.41.3)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.3+snowflake1)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: coloredlogs in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.3)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.31.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.31.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.31.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.31.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.52b0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.52b0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.52b0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.52b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
      "Requirement already satisfied: asgiref~=3.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from opentelemetry-instrumentation-asgi==0.52b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (2.20.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Collecting git+https://github.com/brandonstarxel/chunking_evaluation.git\n",
      "  Cloning https://github.com/brandonstarxel/chunking_evaluation.git to /private/var/folders/_z/ms9rqjt90dq1s5797l_x0ry40000gn/T/pip-req-build-9jlj0qvb\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/brandonstarxel/chunking_evaluation.git /private/var/folders/_z/ms9rqjt90dq1s5797l_x0ry40000gn/T/pip-req-build-9jlj0qvb\n",
      "  Resolved https://github.com/brandonstarxel/chunking_evaluation.git to commit d451fc4cf56e417b755994b4ca5212fd5057c0d2\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tiktoken in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chunking_evaluation==0.1.0) (0.8.0)\n",
      "Requirement already satisfied: fuzzywuzzy in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chunking_evaluation==0.1.0) (0.18.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chunking_evaluation==0.1.0) (2.2.3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chunking_evaluation==0.1.0) (1.23.5)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chunking_evaluation==0.1.0) (4.67.1)\n",
      "Requirement already satisfied: chromadb in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chunking_evaluation==0.1.0) (0.6.3)\n",
      "Requirement already satisfied: python-Levenshtein in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chunking_evaluation==0.1.0) (0.27.1)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chunking_evaluation==0.1.0) (1.59.9)\n",
      "Requirement already satisfied: anthropic in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chunking_evaluation==0.1.0) (0.49.0)\n",
      "Requirement already satisfied: attrs in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chunking_evaluation==0.1.0) (24.3.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from anthropic->chunking_evaluation==0.1.0) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from anthropic->chunking_evaluation==0.1.0) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from anthropic->chunking_evaluation==0.1.0) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from anthropic->chunking_evaluation==0.1.0) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from anthropic->chunking_evaluation==0.1.0) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from anthropic->chunking_evaluation==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from anthropic->chunking_evaluation==0.1.0) (4.12.2)\n",
      "Requirement already satisfied: build>=1.0.3 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb->chunking_evaluation==0.1.0) (1.2.2.post1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb->chunking_evaluation==0.1.0) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb->chunking_evaluation==0.1.0) (0.115.6)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb->chunking_evaluation==0.1.0) (0.34.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb->chunking_evaluation==0.1.0) (3.21.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb->chunking_evaluation==0.1.0) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb->chunking_evaluation==0.1.0) (1.31.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb->chunking_evaluation==0.1.0) (1.31.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb->chunking_evaluation==0.1.0) (0.52b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb->chunking_evaluation==0.1.0) (1.31.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb->chunking_evaluation==0.1.0) (0.21.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb->chunking_evaluation==0.1.0) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb->chunking_evaluation==0.1.0) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb->chunking_evaluation==0.1.0) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb->chunking_evaluation==0.1.0) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb->chunking_evaluation==0.1.0) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb->chunking_evaluation==0.1.0) (0.12.5)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb->chunking_evaluation==0.1.0) (32.0.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb->chunking_evaluation==0.1.0) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb->chunking_evaluation==0.1.0) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb->chunking_evaluation==0.1.0) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb->chunking_evaluation==0.1.0) (3.10.14)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from chromadb->chunking_evaluation==0.1.0) (13.9.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from pandas->chunking_evaluation==0.1.0) (2.8.3+snowflake1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from pandas->chunking_evaluation==0.1.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from pandas->chunking_evaluation==0.1.0) (2023.3)\n",
      "Requirement already satisfied: Levenshtein==0.27.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from python-Levenshtein->chunking_evaluation==0.1.0) (0.27.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from Levenshtein==0.27.1->python-Levenshtein->chunking_evaluation==0.1.0) (3.12.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from tiktoken->chunking_evaluation==0.1.0) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from tiktoken->chunking_evaluation==0.1.0) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from anyio<5,>=3.5.0->anthropic->chunking_evaluation==0.1.0) (3.7)\n",
      "Requirement already satisfied: packaging>=19.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from build>=1.0.3->chromadb->chunking_evaluation==0.1.0) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from build>=1.0.3->chromadb->chunking_evaluation==0.1.0) (1.2.0)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb->chunking_evaluation==0.1.0) (0.41.3)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from httpx<1,>=0.23.0->anthropic->chunking_evaluation==0.1.0) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from httpx<1,>=0.23.0->anthropic->chunking_evaluation==0.1.0) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic->chunking_evaluation==0.1.0) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->chunking_evaluation==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->chunking_evaluation==0.1.0) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->chunking_evaluation==0.1.0) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->chunking_evaluation==0.1.0) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->chunking_evaluation==0.1.0) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->chunking_evaluation==0.1.0) (2.0.7)\n",
      "Requirement already satisfied: durationpy>=0.7 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->chunking_evaluation==0.1.0) (0.9)\n",
      "Requirement already satisfied: coloredlogs in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb->chunking_evaluation==0.1.0) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb->chunking_evaluation==0.1.0) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb->chunking_evaluation==0.1.0) (5.29.3)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb->chunking_evaluation==0.1.0) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb->chunking_evaluation==0.1.0) (1.2.15)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb->chunking_evaluation==0.1.0) (8.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->chunking_evaluation==0.1.0) (1.69.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.31.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->chunking_evaluation==0.1.0) (1.31.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.31.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->chunking_evaluation==0.1.0) (1.31.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.52b0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->chunking_evaluation==0.1.0) (0.52b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.52b0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->chunking_evaluation==0.1.0) (0.52b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->chunking_evaluation==0.1.0) (0.52b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.52b0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->chunking_evaluation==0.1.0) (0.52b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.52b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->chunking_evaluation==0.1.0) (1.14.1)\n",
      "Requirement already satisfied: asgiref~=3.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from opentelemetry-instrumentation-asgi==0.52b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->chunking_evaluation==0.1.0) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb->chunking_evaluation==0.1.0) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb->chunking_evaluation==0.1.0) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->anthropic->chunking_evaluation==0.1.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->anthropic->chunking_evaluation==0.1.0) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken->chunking_evaluation==0.1.0) (3.3.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from rich>=10.11.0->chromadb->chunking_evaluation==0.1.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from rich>=10.11.0->chromadb->chunking_evaluation==0.1.0) (2.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from tokenizers>=0.13.2->chromadb->chunking_evaluation==0.1.0) (0.27.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from typer>=0.9.0->chromadb->chunking_evaluation==0.1.0) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from typer>=0.9.0->chromadb->chunking_evaluation==0.1.0) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb->chunking_evaluation==0.1.0) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb->chunking_evaluation==0.1.0) (0.21.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb->chunking_evaluation==0.1.0) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb->chunking_evaluation==0.1.0) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb->chunking_evaluation==0.1.0) (14.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->chunking_evaluation==0.1.0) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->chunking_evaluation==0.1.0) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->chunking_evaluation==0.1.0) (4.9)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb->chunking_evaluation==0.1.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb->chunking_evaluation==0.1.0) (2023.10.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->chunking_evaluation==0.1.0) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb->chunking_evaluation==0.1.0) (0.1.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb->chunking_evaluation==0.1.0) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb->chunking_evaluation==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/anaconda3/envs/getting_started_llmops/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->chunking_evaluation==0.1.0) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install requests sentence-transformers chromadb openai\n",
    "!pip install git+https://github.com/brandonstarxel/chunking_evaluation.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cad6ae3",
   "metadata": {},
   "source": [
    "Next, let's import the libraries we'll need and define our configuration settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66cc1a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chunking_evaluation.chunking import RecursiveTokenChunker\n",
    "from chunking_evaluation.utils import openai_token_count\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import openai\n",
    "\n",
    "# URL handling\n",
    "JINA_PREFIX = \"https://r.jina.ai/\"\n",
    "\n",
    "# Chunking settings\n",
    "CHUNK_SIZE = 400\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "# Embedding settings\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# ChromaDB settings\n",
    "COLLECTION_NAME = \"rag_documents\"\n",
    "PERSIST_DIRECTORY = \"./chroma_db\"\n",
    "\n",
    "# OpenAI settings\n",
    "OPENAI_MODEL = \"gpt-3.5-turbo\"\n",
    "# Replace with your actual API key\n",
    "OPENAI_API_KEY = \"your-api-key-here\"  \n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Retrieval settings\n",
    "TOP_K = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8bc44c",
   "metadata": {},
   "source": [
    "## 1. Loading Documents from URLs\n",
    "\n",
    "Let's start by implementing a function to fetch content from web URLs using the jina.ai reader service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d04b19dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_url_content(url):\n",
    "    \"\"\"\n",
    "    Fetch content from a URL using jina.ai reader service.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The original URL to fetch\n",
    "        \n",
    "    Returns:\n",
    "        str: The content of the URL in markdown format\n",
    "    \"\"\"\n",
    "    # Prepend the jina.ai prefix to the URL\n",
    "    jina_url = JINA_PREFIX + url\n",
    "    \n",
    "    try:\n",
    "        # Make the request to jina.ai reader service\n",
    "        response = requests.get(jina_url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            print(f\"Error fetching URL: Status code {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fbb6c1",
   "metadata": {},
   "source": [
    "Now let's test our function by fetching content from a sample URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "063e4c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content length: 27961 characters\n",
      "\n",
      "Preview of the first 500 characters:\n",
      "Title: Retrieval-augmented generation\n",
      "\n",
      "URL Source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation\n",
      "\n",
      "Published Time: 2023-11-05T13:19:20Z\n",
      "\n",
      "Markdown Content:\n",
      "From Wikipedia, the free encyclopedia\n",
      "\n",
      "**Retrieval-augmented generation** (**RAG**) is a technique that enables [generative artificial intelligence](https://en.wikipedia.org/wiki/Generative_artificial_intelligence \"Generative artificial intelligence\") (Gen AI) models to retrieve and incorporate new information.[\\[1\\]](https://en....\n"
     ]
    }
   ],
   "source": [
    "# Example URL (a Wikipedia article)\n",
    "url = \"https://en.wikipedia.org/wiki/Retrieval-augmented_generation\"\n",
    "\n",
    "# Fetch the content\n",
    "content = fetch_url_content(url)\n",
    "\n",
    "# Print a preview of the content\n",
    "if content:\n",
    "    print(f\"Content length: {len(content)} characters\")\n",
    "    print(\"\\nPreview of the first 500 characters:\")\n",
    "    print(content[:500] + \"...\")\n",
    "else:\n",
    "    print(\"Failed to fetch content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb3df9b",
   "metadata": {},
   "source": [
    "## 2. Chunking the Content\n",
    "\n",
    "Now that we can fetch content, let's implement the chunking functionality using RecursiveTokenChunker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30be2d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_document(text):\n",
    "    \"\"\"\n",
    "    Split document text into chunks using RecursiveTokenChunker.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The document text to chunk\n",
    "        \n",
    "    Returns:\n",
    "        list: List of text chunks\n",
    "    \"\"\"\n",
    "    # Initialize the chunker with our configured settings\n",
    "    chunker = RecursiveTokenChunker(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=openai_token_count,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = chunker.split_text(text)\n",
    "    \n",
    "    # Return the chunks\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c46b91",
   "metadata": {},
   "source": [
    "Let's test our chunking function on the content we fetched:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "704cf7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original content length: 27961 characters\n",
      "Number of chunks: 22\n",
      "Average chunk length: 1283.0 characters\n",
      "\n",
      "Sample chunk:\n",
      "--------------------------------------------------\n",
      "Typically, the data to be referenced is converted into LLM [embeddings](https://en.wikipedia.org/wiki/Word_embeddings \"Word embeddings\"), numerical representations in the form of a large vector space. RAG can be used on unstructured (usually text), semi-structured, or structured data (for example [knowledge graphs](https://en.wikipedia.org/wiki/Knowledge_graphs \"Knowledge graphs\")).[\\[3\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-Survey-3) These embeddings are then stored in a [vector database](https://en.wikipedia.org/wiki/Vector_database \"Vector database\") to allow for [document retrieval](https://en.wikipedia.org/wiki/Document_retrieval \"Document retrieval\").\n",
      "\n",
      "[![Image 1](https://upload.wikimedia.org/wikipedia/commons/thumb/1/14/RAG_diagram.svg/220px-RAG_diagram.svg.png)](https://en.wikipedia.org/wiki/File:RAG_diagram.svg)\n",
      "\n",
      "Overview of RAG process, combining external documents and user input into an LLM prompt to get tailored output\n",
      "\n",
      "Given a user query, a document retriever is first called to select the most relevant documents that will be used to augment the query.[\\[2\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:2-2) This comparison can be done using a variety of methods, which depend in part on the type of indexing used.[\\[1\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:1-1)[\\[3\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-Survey-3)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if content:\n",
    "    # Chunk the content\n",
    "    chunks = chunk_document(content)\n",
    "    \n",
    "    # Print chunking statistics\n",
    "    print(f\"Original content length: {len(content)} characters\")\n",
    "    print(f\"Number of chunks: {len(chunks)}\")\n",
    "    print(f\"Average chunk length: {sum(len(c) for c in chunks) / len(chunks):.1f} characters\")\n",
    "    \n",
    "    # Print a sample chunk\n",
    "    if chunks:\n",
    "        print(\"\\nSample chunk:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(chunks[min(5, len(chunks)-1)])  # Print the 6th chunk or the last one if fewer\n",
    "        print(\"-\" * 50)\n",
    "else:\n",
    "    print(\"No content to chunk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc88091",
   "metadata": {},
   "source": [
    "## 3. Embedding Chunks\n",
    "\n",
    "Now let's implement the functionality to create embeddings for our text chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bb92740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "def embed_texts(texts):\n",
    "    \"\"\"\n",
    "    Create embeddings for a list of text chunks.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of text strings to embed\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Array of embeddings\n",
    "    \"\"\"\n",
    "    embeddings = embedding_model.encode(texts)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd242539",
   "metadata": {},
   "source": [
    "Let's test our embedding function on a few chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cd6cc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimensions: (3, 384)\n",
      "Sample embedding (first 10 values): [-0.15525298 -0.00383533 -0.04336745  0.09886687 -0.01397708  0.06526825\n",
      "  0.03533417 -0.01896184 -0.00340518 -0.04793883]\n",
      "\n",
      "Similarity between first two chunks: 0.7273\n"
     ]
    }
   ],
   "source": [
    "if chunks:\n",
    "    # Create embeddings for the first 3 chunks\n",
    "    sample_chunks = chunks[:min(3, len(chunks))]\n",
    "    embeddings = embed_texts(sample_chunks)\n",
    "    \n",
    "    # Print embedding information\n",
    "    print(f\"Embedding dimensions: {embeddings.shape}\")\n",
    "    print(f\"Sample embedding (first 10 values): {embeddings[0][:10]}\")\n",
    "    \n",
    "    # Calculate similarity between the first two chunks if possible\n",
    "    if len(embeddings) >= 2:\n",
    "        # Using cosine similarity: dot product of normalized vectors\n",
    "        def cosine_similarity(v1, v2):\n",
    "            return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "        \n",
    "        sim = cosine_similarity(embeddings[0], embeddings[1])\n",
    "        print(f\"\\nSimilarity between first two chunks: {sim:.4f}\")\n",
    "else:\n",
    "    print(\"No chunks to embed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa25261",
   "metadata": {},
   "source": [
    "## 4. Creating a ChromaDB Vector Store\n",
    "\n",
    "Now let's implement the functionality to store our embeddings in ChromaDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a918850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_collection():\n",
    "    \"\"\"\n",
    "    Get or create a ChromaDB collection.\n",
    "    \n",
    "    Returns:\n",
    "        chromadb.Collection: The ChromaDB collection\n",
    "    \"\"\"\n",
    "    # Create the persistent client\n",
    "    client = chromadb.PersistentClient(path=PERSIST_DIRECTORY)\n",
    "    \n",
    "    # Set up the embedding function\n",
    "    embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=EMBEDDING_MODEL\n",
    "    )\n",
    "    \n",
    "    # Try to get the collection if it exists\n",
    "    try:\n",
    "        collection = client.get_collection(\n",
    "            name=COLLECTION_NAME,\n",
    "            embedding_function=embedding_func\n",
    "        )\n",
    "        print(f\"Using existing collection: {COLLECTION_NAME}\")\n",
    "    except:\n",
    "        # Create a new collection if it doesn't exist\n",
    "        collection = client.create_collection(\n",
    "            name=COLLECTION_NAME,\n",
    "            embedding_function=embedding_func,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}  # Use cosine similarity\n",
    "        )\n",
    "        print(f\"Created new collection: {COLLECTION_NAME}\")\n",
    "    \n",
    "    return collection\n",
    "\n",
    "def add_chunks_to_collection(chunks, url):\n",
    "    \"\"\"\n",
    "    Add document chunks to the ChromaDB collection.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): List of text chunks\n",
    "        url (str): Source URL of the document\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of chunks added\n",
    "    \"\"\"\n",
    "    # Get the collection\n",
    "    collection = get_or_create_collection()\n",
    "    \n",
    "    # Skip if no chunks\n",
    "    if not chunks:\n",
    "        return 0\n",
    "    \n",
    "    # Create IDs for each chunk\n",
    "    ids = [f\"chunk_{url.replace('/', '_')}_{i}\" for i in range(len(chunks))]\n",
    "    \n",
    "    # Create metadata for each chunk\n",
    "    metadatas = [{\"source\": url, \"chunk_index\": i} for i in range(len(chunks))]\n",
    "    \n",
    "    # Add chunks to the collection\n",
    "    collection.add(\n",
    "        documents=chunks,\n",
    "        ids=ids,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "    \n",
    "    return len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93833f1",
   "metadata": {},
   "source": [
    "Let's test adding chunks to our vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "def96f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new collection: rag_documents\n",
      "Added 22 chunks to the collection\n",
      "Using existing collection: rag_documents\n",
      "Total documents in collection: 22\n"
     ]
    }
   ],
   "source": [
    "if chunks:\n",
    "    # Add chunks to the collection\n",
    "    num_added = add_chunks_to_collection(chunks, url)\n",
    "    print(f\"Added {num_added} chunks to the collection\")\n",
    "    \n",
    "    # Get the collection and check count\n",
    "    collection = get_or_create_collection()\n",
    "    count = collection.count()\n",
    "    print(f\"Total documents in collection: {count}\")\n",
    "else:\n",
    "    print(\"No chunks to add\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbb5c4",
   "metadata": {},
   "source": [
    "## 5. Implementing the Retriever\n",
    "\n",
    "Now let's implement a function to retrieve the most relevant chunks for a given query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81e2d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query):\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant document chunks for a query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The query text\n",
    "        \n",
    "    Returns:\n",
    "        list: List of relevant document chunks\n",
    "        list: List of source URLs for each chunk\n",
    "    \"\"\"\n",
    "    # Get the collection\n",
    "    collection = get_or_create_collection()\n",
    "    \n",
    "    # Query the collection for similar chunks\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=TOP_K,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    # Extract the retrieved chunks and their sources\n",
    "    chunks = results[\"documents\"][0]  # First list is for the first query\n",
    "    metadatas = results[\"metadatas\"][0]\n",
    "    distances = results[\"distances\"][0]\n",
    "    \n",
    "    # Extract the sources\n",
    "    sources = [meta[\"source\"] for meta in metadatas]\n",
    "    \n",
    "    # Print retrieval information for debugging\n",
    "    print(f\"Retrieved {len(chunks)} chunks for query: '{query}'\")\n",
    "    for i, (chunk, source, distance) in enumerate(zip(chunks, sources, distances)):\n",
    "        print(\"-\"*40)\n",
    "        print(f\"\\nChunk {i+1} (Distance: {distance:.4f}, Source: {source}):\")\n",
    "        preview = chunk\n",
    "        print(preview)\n",
    "    \n",
    "    return chunks, sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb213d11",
   "metadata": {},
   "source": [
    "Let's test our retriever with a sample query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3f5685a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing collection: rag_documents\n",
      "Retrieved 5 chunks for query: 'What are the advantages of retrieval augmented generation?'\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 1 (Distance: 0.3417, Source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation):\n",
      "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by incorporating an [information-retrieval](https://en.wikipedia.org/wiki/Information_retrieval \"Information retrieval\") mechanism that allows models to access and utilize additional data beyond their original training set. [AWS](https://en.wikipedia.org/wiki/AWS \"AWS\") states, \"RAG allows LLMs to retrieve relevant information from external data sources to generate more accurate and contextually relevant responses\" (**indexing**).[\\[4\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-AWS-4) This approach reduces reliance on static datasets, which can quickly become outdated. When a user submits a query, RAG uses a document retriever to search for relevant content from available sources before incorporating the retrieved information into the models response (**retrieval**). _Ars Technica_ notes that \"when new information becomes available, rather than having to retrain the model, all thats needed is to augment the models external knowledge base with the updated information\" (**augmentation**).[\\[5\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:0-5) By dynamically integrating relevant data, RAG enables LLMs to generate more informed and contextually grounded responses (**generation**). IBM states that \"in the generative phase, the LLM draws from the augmented prompt and its internal representation of its training data to synthesize an engaging answer tailored to the user in that instant.[\\[1\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:1-1)\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 2 (Distance: 0.3625, Source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation):\n",
      "Title: Retrieval-augmented generation\n",
      "\n",
      "URL Source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation\n",
      "\n",
      "Published Time: 2023-11-05T13:19:20Z\n",
      "\n",
      "Markdown Content:\n",
      "From Wikipedia, the free encyclopedia\n",
      "\n",
      "**Retrieval-augmented generation** (**RAG**) is a technique that enables [generative artificial intelligence](https://en.wikipedia.org/wiki/Generative_artificial_intelligence \"Generative artificial intelligence\") (Gen AI) models to retrieve and incorporate new information.[\\[1\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:1-1) It modifies interactions with a [large language model](https://en.wikipedia.org/wiki/Large_language_model \"Large language model\") (LLM) so that the model responds to user queries with reference to a specified set of documents, using this information to supplement information from its pre-existing [training data](https://en.wikipedia.org/wiki/Training_data \"Training data\").[\\[2\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:2-2) This allows LLMs to use domain-specific and/or updated information.[\\[2\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:2-2)[\\[3\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-Survey-3) Use cases include providing [chatbot](https://en.wikipedia.org/wiki/Chatbot \"Chatbot\") access to internal company data or generating responses based on authoritative sources.[\\[4\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-AWS-4)\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 3 (Distance: 0.3988, Source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation):\n",
      "7.  **[^](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_ref-BUZBP_7-0)** Lewis, Patrick; Perez, Ethan; Piktus, Aleksandra; Petroni, Fabio; Karpukhin, Vladimir; Goyal, Naman; Kttler, Heinrich; Lewis, Mike; Yih, Wen-tau; Rocktschel, Tim; Riedel, Sebastian; Kiela, Douwe (2020). [\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html). _Advances in Neural Information Processing Systems_. **33**. Curran Associates, Inc.: 94599474\\. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2005.11401](https://arxiv.org/abs/2005.11401).\n",
      "8.  ^ [_**a**_](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_ref-:3_8-0) [_**b**_](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_ref-:3_8-1) Luan, Yi; Eisenstein, Jacob; Toutanova, Kristina; Collins, Michael (26 April 2021). [\"Sparse, Dense, and Attentional Representations for Text Retrieval\"](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00369/100684/Sparse-Dense-and-Attentional-Representations-for). _MIT Press Direct_. Retrieved 15 March 2025.\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 4 (Distance: 0.4053, Source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation):\n",
      "These methods aim to enhance the quality of document retrieval in vector databases:\n",
      "\n",
      "*   Pre-training the retriever using the _Inverse Cloze Task_ (ICT), a technique that helps the model learn retrieval patterns by predicting masked text within documents.[\\[12\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-orqa-12)\n",
      "*   Progressive data augmentation, as used in _Diverse Augmentation for Generalizable Dense Retrieval_ (DRAGON), improves dense retrieval by sampling difficult negative examples during training.[\\[13\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-dragon-13)\n",
      "*   Supervised retriever optimization aligns retrieval probabilities with the generator models likelihood distribution. This involves retrieving the top-k vectors for a given prompt, scoring the generated responses [perplexity](https://en.wikipedia.org/wiki/Perplexity \"Perplexity\"), and minimizing [KL divergence](https://en.wikipedia.org/wiki/KL_divergence \"KL divergence\") between the retrievers selections and the models likelihoods to refine retrieval.[\\[14\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-14)\n",
      "*   Reranking techniques can refine retriever performance by prioritizing the most relevant retrieved documents during training.[\\[15\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-15)\n",
      "\n",
      "[![Image 2](https://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Language_model_in_Deepmind%27s_2021_Retro_for_RAG.svg/300px-Language_model_in_Deepmind%27s_2021_Retro_for_RAG.svg.png)](https://en.wikipedia.org/wiki/File:Language_model_in_Deepmind%27s_2021_Retro_for_RAG.svg)\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 5 (Distance: 0.4142, Source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation):\n",
      "1.  ^ [_**a**_](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_ref-:1_1-0) [_**b**_](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_ref-:1_1-1) [_**c**_](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_ref-:1_1-2) [_**d**_](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_ref-:1_1-3) [_**e**_](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_ref-:1_1-4) [_**f**_](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_ref-:1_1-5) [\"What is retrieval-augmented generation?\"](https://research.ibm.com/blog/retrieval-augmented-generation-RAG). _IBM_. 22 August 2023. Retrieved 7 March 2025.\n",
      "\n",
      "Retrieved 5 chunks from 1 unique sources\n"
     ]
    }
   ],
   "source": [
    "# Define a test query\n",
    "query = \"What are the advantages of retrieval augmented generation?\"\n",
    "\n",
    "# Retrieve relevant chunks\n",
    "chunks, sources = retrieve_relevant_chunks(query)\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nRetrieved {len(chunks)} chunks from {len(set(sources))} unique sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c12c1",
   "metadata": {},
   "source": [
    "## 6. Generating Responses with OpenAI\n",
    "\n",
    "Finally, let's implement a function to generate responses using OpenAI's API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fb4cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def generate_response(query, chunks, sources):\n",
    "    \"\"\"\n",
    "    Generate a response using OpenAI's API based on the query and retrieved chunks.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's query\n",
    "        chunks (list): List of relevant document chunks\n",
    "        sources (list): List of source URLs for each chunk\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated response\n",
    "    \"\"\"\n",
    "    # Combine chunks with their sources for better attribution\n",
    "    context_with_sources = []\n",
    "    for i, (chunk, source) in enumerate(zip(chunks, sources)):\n",
    "        context_with_sources.append(f\"Source [{i+1}] ({source}): {chunk}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_with_sources)\n",
    "    \n",
    "    # Define system message with instructions\n",
    "    system_message = \"\"\"You are a helpful assistant that provides accurate information based on the given context. \n",
    "    If the context doesn't contain relevant information to answer the question, acknowledge that and provide general information if possible.\n",
    "    Always cite your sources by referring to the source numbers provided in brackets. Do not make up information.\"\"\"\n",
    "    \n",
    "    # Define the user message with query and context\n",
    "    user_message = f\"\"\"Question: {query}\n",
    "    \n",
    "    Context information:\n",
    "    {context}\n",
    "    \n",
    "    Please answer the question based on the context information provided.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Call the OpenAI API\n",
    "        client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            temperature=0.3,  # Lower temperature for more focused responses\n",
    "            max_tokens=1000   # Limit response length\n",
    "        )\n",
    "        \n",
    "        # Extract and return the response text\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return f\"Error generating response: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c193229",
   "metadata": {},
   "source": [
    "Let's test our response generation with the retrieved chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d16be4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "--------------------------------------------------------------------------------\n",
      "The advantages of retrieval-augmented generation (RAG) include:\n",
      "\n",
      "1. **Access to Additional Data**: RAG allows large language models (LLMs) to retrieve relevant information from external data sources beyond their original training set, enabling them to generate more accurate and contextually relevant responses (indexing) [1].\n",
      "   \n",
      "2. **Reduced Reliance on Static Datasets**: By incorporating an information-retrieval mechanism, RAG reduces the reliance on static datasets that can quickly become outdated. When new information becomes available, the model can be augmented with updated information without the need for retraining (augmentation) [1].\n",
      "\n",
      "3. **Enhanced Responses**: RAG enables LLMs to dynamically integrate relevant data, leading to more informed and contextually grounded responses (generation) [1].\n",
      "\n",
      "4. **Domain-specific and Updated Information**: RAG allows LLMs to use domain-specific and/or updated information, enhancing the quality of responses provided by the models [2].\n",
      "\n",
      "5. **Improved Document Retrieval**: RAG methods enhance the quality of document retrieval in vector databases through techniques like pre-training the retriever, progressive data augmentation, supervised retriever optimization, and reranking techniques [4].\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if chunks:\n",
    "    # Generate a response\n",
    "    response = generate_response(query, chunks, sources)\n",
    "    print(\"\\nGenerated Response:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(response)\n",
    "    print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"No chunks retrieved, cannot generate response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ad003e",
   "metadata": {},
   "source": [
    "## 7. Putting It All Together: The Complete RAG Pipeline\n",
    "\n",
    "Let's combine everything into a complete RAG pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6508455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(url, query):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: fetch a document, chunk it, add to vector store, and answer a query.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to fetch and add to knowledge base\n",
    "        query (str): Query to answer using the RAG system\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    print(f\"Step 1: Fetching document from {url}\")\n",
    "    content = fetch_url_content(url)\n",
    "    if not content:\n",
    "        return \"Failed to fetch the document.\"\n",
    "    \n",
    "    print(f\"\\nStep 2: Chunking the document\")\n",
    "    chunks = chunk_document(content)\n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    \n",
    "    print(f\"\\nStep 3: Adding chunks to vector store\")\n",
    "    num_added = add_chunks_to_collection(chunks, url)\n",
    "    print(f\"Added {num_added} chunks to vector store\")\n",
    "    \n",
    "    print(f\"\\nStep 4: Retrieving relevant chunks for query: '{query}'\")\n",
    "    relevant_chunks, sources = retrieve_relevant_chunks(query)\n",
    "    \n",
    "    print(f\"\\nStep 5: Generating response\")\n",
    "    if relevant_chunks:\n",
    "        response = generate_response(query, relevant_chunks, sources)\n",
    "        return response\n",
    "    else:\n",
    "        return \"No relevant information found to answer the query.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281caa5",
   "metadata": {},
   "source": [
    "Let's test the entire pipeline with a new document and query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbdbbcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Fetching document from https://en.wikipedia.org/wiki/Large_language_model\n",
      "\n",
      "Step 2: Chunking the document\n",
      "Created 68 chunks\n",
      "\n",
      "Step 3: Adding chunks to vector store\n",
      "Using existing collection: rag_documents\n",
      "Added 68 chunks to vector store\n",
      "\n",
      "Step 4: Retrieving relevant chunks for query: 'How do large language models work and what are their limitations?'\n",
      "Using existing collection: rag_documents\n",
      "Retrieved 5 chunks for query: 'How do large language models work and what are their limitations?'\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 1 (Distance: 0.3294, Source: https://en.wikipedia.org/wiki/Large_language_model):\n",
      "^ A Closer Look at Large Language Models Emergent Abilities Archived 2023-06-24 at the Wayback Machine (Yao Fu, Nov 20, 2022)\n",
      "^ Ornes, Stephen (March 16, 2023). \"The Unpredictable Abilities Emerging From Large AI Models\". Quanta Magazine. Archived from the original on March 16, 2023. Retrieved March 16, 2023.\n",
      "^ Schaeffer, Rylan; Miranda, Brando; Koyejo, Sanmi (2023-04-01). \"Are Emergent Abilities of Large Language Models a Mirage?\". arXiv:2304.15004 [cs.AI].\n",
      "^ Blank, Idan A. (November 2023). \"What are large language models supposed to model?\". Trends in Cognitive Sciences. 27 (11): 987989. doi:10.1016/j.tics.2023.08.006. PMID37659920.\n",
      "^ Li, Kenneth; Hopkins, Aspen K.; Bau, David; Vigas, Fernanda; Pfister, Hanspeter; Wattenberg, Martin (2022-10-01). \"Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task\". arXiv:2210.13382 [cs.LG].\n",
      "^ \"Large Language Model: world models or surface statistics?\". The Gradient. 2023-01-21. Retrieved 2023-06-12.\n",
      "^ Jin, Charles; Rinard, Martin (2023-05-01). \"Evidence of Meaning in Language Models Trained on Programs\". arXiv:2305.11169 [cs.LG].\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 2 (Distance: 0.3529, Source: https://en.wikipedia.org/wiki/Large_language_model):\n",
      "Further reading[edit]\n",
      "Jurafsky, Dan, Martin, James. H. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, 3rd Edition draft, 2023.\n",
      "Zhao, Wayne Xin; etal. (2023). \"A Survey of Large Language Models\". arXiv:2303.18223 [cs.CL].\n",
      "Kaddour, Jean; etal. (2023). \"Challenges and Applications of Large Language Models\". arXiv:2307.10169 [cs.CL].\n",
      "Yin, Shukang; Fu, Chaoyou; Zhao, Sirui; Li, Ke; Sun, Xing; Xu, Tong; Chen, Enhong (2024). \"A Survey on Multimodal Large Language Models\". National Science Review. 11 (12): nwae403. arXiv:2306.13549. doi:10.1093/nsr/nwae403. PMC11645129. PMID39679213.\n",
      "\"AI Index Report 2024  Artificial Intelligence Index\". aiindex.stanford.edu. Retrieved 2024-05-05.\n",
      "Frank, Michael C. (27 June 2023). \"Baby steps in evaluating the capacities of large language models\". Nature Reviews Psychology. 2 (8): 451452. doi:10.1038/s44159-023-00211-x. ISSN2731-0574. S2CID259713140. Retrieved 2 July 2023.\n",
      "vte\n",
      "Natural language processing\n",
      "\n",
      "General terms\t\n",
      "AI-completeBag-of-wordsn-gram BigramTrigramComputational linguisticsNatural language understandingStop wordsText processing\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 3 (Distance: 0.3587, Source: https://en.wikipedia.org/wiki/Large_language_model):\n",
      "History[edit]\n",
      "The training compute of notable large models in FLOPs vs publication date over the period 2010-2024. For overall notable models (top left), frontier models (top right), top language models (bottom left) and top models within leading companies (bottom right). The majority of these models are language models.\n",
      "The training compute of notable large AI models in FLOPs vs publication date over the period 2017-2024. The majority of large models are language models or multimodal models with language capacity.\n",
      "\n",
      "Before 2017, there were a few language models that were large as compared to capacities then available. In the 1990s, the IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion words achieved state-of-the-art perplexity at the time.[4] In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets (\"web as corpus\"[5]), upon which they trained statistical language models.[6][7] In 2009, in most language processing tasks, statistical language models dominated over symbolic language models because they can usefully ingest large datasets.[8]\n",
      "\n",
      "After neural networks became dominant in image processing around 2012,[9] they were applied to language modelling as well. Google converted its translation service to Neural Machine Translation in 2016. Because it preceded the existence of transformers, it was done by seq2seq deep LSTM networks.\n",
      "\n",
      "An illustration of main components of the transformer model from the original paper, where layers were normalized after (instead of before) multiheaded attention\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 4 (Distance: 0.4019, Source: https://en.wikipedia.org/wiki/Large_language_model):\n",
      "^ \"PAL: Program-aided Language Models\". reasonwithpal.com. Archived from the original on 2023-06-12. Retrieved 2023-06-12.\n",
      "^ Paranjape, Bhargavi; Lundberg, Scott; Singh, Sameer; Hajishirzi, Hannaneh; Zettlemoyer, Luke; Tulio Ribeiro, Marco (2023-03-01). \"ART: Automatic multi-step reasoning and tool-use for large language models\". arXiv:2303.09014 [cs.CL].\n",
      "^ Liang, Yaobo; Wu, Chenfei; Song, Ting; Wu, Wenshan; Xia, Yan; Liu, Yu; Ou, Yang; Lu, Shuai; Ji, Lei; Mao, Shaoguang; Wang, Yun; Shou, Linjun; Gong, Ming; Duan, Nan (2023-03-01). \"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs\". arXiv:2303.16434 [cs.AI].\n",
      "^ Patil, Shishir G.; Zhang, Tianjun; Wang, Xin; Gonzalez, Joseph E. (2023-05-01). \"Gorilla: Large Language Model Connected with Massive APIs\". arXiv:2305.15334 [cs.CL].\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 5 (Distance: 0.4058, Source: https://en.wikipedia.org/wiki/Large_language_model):\n",
      "Large language models by themselves are black boxes, and it is not clear how they can perform linguistic tasks. Similarly, it is unclear if or how LLMs should be viewed as models of the human brain and/or human mind.[105]\n",
      "\n",
      "There are several methods for understanding how LLMs work. Mechanistic interpretability aims to reverse-engineer LLMs by discovering symbolic algorithms that approximate the inference performed by an LLM. One example is Othello-GPT, where a small Transformer is trained to predict legal Othello moves. It is found that there is a linear representation of the Othello board, and modifying the representation changes the predicted legal Othello moves in the correct way.[106][107] In another example, a small Transformer is trained on Karel programs. Similar to the Othello-GPT example, there is a linear representation of Karel program semantics, and modifying the representation changes output in the correct way. The model also generates correct programs that are on average shorter than those in the training set.[108]\n",
      "\n",
      "In another example, the authors trained small transformers on modular arithmetic addition. The resulting models were reverse-engineered, and it turned out they used discrete Fourier transform.[109]\n",
      "\n",
      "A related concept is AI explainability, which focuses on understanding how an AI model arrives at a given result.\n",
      "\n",
      "Understanding and intelligence[edit]\n",
      "See also: Philosophy of artificial intelligence and Artificial consciousness\n",
      "\n",
      "Step 5: Generating response\n",
      "\n",
      "Final Answer:\n",
      "================================================================================\n",
      "Large language models work by utilizing neural networks to process and generate human language text. They are trained on vast amounts of text data to learn patterns and relationships within the language, enabling them to generate coherent and contextually relevant text. These models have shown emergent abilities in various linguistic tasks and have been applied in natural language processing tasks such as translation, summarization, and question-answering [1].\n",
      "\n",
      "However, large language models also have limitations. They are often considered black boxes, meaning it is challenging to understand how they arrive at specific results or perform linguistic tasks. It is unclear whether or how these models should be viewed as representations of the human brain or mind [5]. Efforts are being made to enhance the interpretability of these models through methods like mechanistic interpretability and AI explainability to better understand their inner workings [5].\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define a new URL and query\n",
    "new_url = \"https://en.wikipedia.org/wiki/Large_language_model\"\n",
    "new_query = \"How do large language models work and what are their limitations?\"\n",
    "\n",
    "# Run the RAG pipeline\n",
    "answer = rag_pipeline(new_url, new_query)\n",
    "\n",
    "# Print the response\n",
    "print(\"\\nFinal Answer:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2dc03",
   "metadata": {},
   "source": [
    "## 8. Testing Multiple Queries\n",
    "\n",
    "Let's try a few more queries to test our RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fe80dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 1: What are the key challenges in implementing RAG systems?\n",
      "--------------------------------------------------------------------------------\n",
      "Using existing collection: rag_documents\n",
      "Retrieved 5 chunks for query: 'What are the key challenges in implementing RAG systems?'\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 1 (Distance: 0.4109, Source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation):\n",
      "RAG is not a complete solution to the problem of hallucinations in LLMs. According to [_Ars Technica_](https://en.wikipedia.org/wiki/Ars_Technica \"Ars Technica\"), \"It is not a direct solution because the LLM can still hallucinate around the source material in its response.\"[\\[5\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:0-5)\n",
      "\n",
      "While RAG improves the accuracy of large language models (LLMs), it does not eliminate all challenges. One limitation is that while RAG reduces the need for frequent model retraining, it does not remove it entirely. Additionally, LLMs may struggle to recognize when they lack sufficient information to provide a reliable response. Without specific training, models may generate answers even when they should indicate uncertainty. According to [IBM](https://en.wikipedia.org/wiki/IBM \"IBM\"), this issue can arise when the model lacks the ability to assess its own knowledge limitations.[\\[1\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:1-1)\n",
      "\n",
      "RAG systems may retrieve factually correct but misleading sources, leading to errors in interpretation. In some cases, an LLM may extract statements from a source without considering its context, resulting in an incorrect conclusion. Additionally, when faced with conflicting information, RAG models may struggle to determine which source is accurate and may combine details from multiple sources, producing responses that merge outdated and updated information in a misleading way. According to the [_MIT Technology Review_,](https://en.wikipedia.org/wiki/MIT_Technology_Review \"MIT Technology Review\") these issues occur because RAG systems may misinterpret the data they retrieve.[\\[2\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:2-2)\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 2 (Distance: 0.5435, Source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation):\n",
      "RAG improves large language models (LLMs) by incorporating [information retrieval](https://en.wikipedia.org/wiki/Information_retrieval \"Information retrieval\") before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources.[\\[1\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:1-1) According to _[Ars](https://en.wikipedia.org/wiki/Ars_Technica \"Ars Technica\") [Technica](https://en.wikipedia.org/wiki/Ars_Technica \"Ars Technica\")_, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce [AI hallucinations](https://en.wikipedia.org/wiki/AI_hallucinations \"AI hallucinations\"), which have led to real-world issues like chatbots inventing policies or lawyers citing nonexistent legal cases.[\\[5\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:0-5)\n",
      "\n",
      "By dynamically retrieving information, RAG enables AI to provide more accurate responses without frequent retraining. According to [IBM](https://en.wikipedia.org/wiki/IBM \"IBM\"), \"RAG also reduces the need for users to continuously train the model on new data and update its parameters as circumstances evolve. In this way, RAG can lower the computational and financial costs of running LLM-powered [chatbots](https://en.wikipedia.org/wiki/Chatbot \"Chatbot\") in an enterprise setting.\"[\\[1\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:1-1)\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 3 (Distance: 0.5680, Source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation):\n",
      "RAG systems do not inherently verify the credibility of the sources they retrieve from, which can lead to misleading or inaccurate responses. AI systems can generate misinformation even when pulling from factually correct sources if they misinterpret the context. [_MIT Technology Review_](https://en.wikipedia.org/wiki/MIT_Technology_Review \"MIT Technology Review\") gives the example of an AI-generated response stating, \"The United States has had one Muslim president, Barack Hussein Obama.\" The model retrieved this from an academic book titled _Barack Hussein Obama: Americas First Muslim President?_ and misinterpreted the content, generating a false claim.[\\[2\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:2-2)\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) is a method that allows large language models (LLMs) to retrieve and incorporate additional information before generating responses. Unlike LLMs that rely solely on pre-existing [training data](https://en.wikipedia.org/wiki/Training_data \"Training data\"), RAG integrates newly available data at query time. _Ars Technica_ states, \"The beauty of RAG is that when new information becomes available, rather than having to retrain the model, all thats needed is to augment the models external knowledge base with the updated information.\"[\\[5\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:0-5)\n",
      "\n",
      "The _BBC_ describes \"prompt stuffing\" as a technique within RAG, in which relevant context is inserted into a prompt to guide the models response. This approach provides the LLM with key information early in the prompt, encouraging it to prioritize the supplied data over pre-existing training knowledge.[\\[6\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-6)\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 4 (Distance: 0.5925, Source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation):\n",
      "Retro language model for RAG. Each Retro block consists of Attention, Chunked Cross Attention, and Feed Forward layers. Black-lettered boxes show data being changed, and blue lettering shows the algorithm performing the changes.\n",
      "\n",
      "By redesigning the language model with the retriever in mind, a 25-time smaller network can get comparable perplexity as its much larger counterparts.[\\[16\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-borgeaud-16) Because it is trained from scratch, this method (Retro) incurs the high cost of training runs that the original RAG scheme avoided. The hypothesis is that by giving domain knowledge during training, Retro needs less focus on the domain and can devote its smaller weight resources only to language semantics. The redesigned language model is shown here.\n",
      "\n",
      "It has been reported that Retro is not reproducible, so modifications were made to make it so. The more reproducible version is called Retro++ and includes in-context RAG.[\\[17\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-wang2023a-17)\n",
      "\n",
      "Chunking involves various strategies for breaking up the data into vectors so the retriever can find details in it.\n",
      "\n",
      "[![Image 3](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0c/Rag-doc-styles.png/500px-Rag-doc-styles.png)](https://en.wikipedia.org/wiki/File:Rag-doc-styles.png)\n",
      "\n",
      "Different data styles have patterns that correct chunking can take advantage of.\n",
      "\n",
      "Three types of chunking strategies are:\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 5 (Distance: 0.6316, Source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation):\n",
      "Beyond efficiency gains, RAG also allows LLMs to include source references in their responses, enabling users to verify information by reviewing cited documents or original sources. This can provide greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.[\\[4\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-AWS-4)\n",
      "\n",
      "RAG and LLM Limitations\n",
      "-----------------------\n",
      "\n",
      "\\[[edit](https://en.wikipedia.org/w/index.php?title=Retrieval-augmented_generation&action=edit&section=1 \"Edit section: RAG and LLM Limitations\")\\]\n",
      "\n",
      "In June 2024, _Ars Technica_ reported, \"But LLMs arent humans, of course. Their training data can age quickly, particularly in more time-sensitive queries. In addition, the LLM often cant distinguish specific sources of its knowledge, as all its training data is blended together into a kind of soup.\" In 2023, during its launch demonstration, [Googles Bard](https://en.wikipedia.org/wiki/Google_Bard \"Google Bard\") provided incorrect information about the [James Webb Space Telescope](https://en.wikipedia.org/wiki/James_Webb_Space_Telescope \"James Webb Space Telescope\"), an error that contributed to a $100 billion decline in [Alphabet](https://en.wikipedia.org/wiki/Alphabet_Inc. \"Alphabet Inc.\")s stock value.[\\[5\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:0-5)\n",
      "\n",
      "Answer:\n",
      "Key challenges in implementing RAG systems include:\n",
      "\n",
      "1. **Hallucinations in LLMs**: RAG is not a complete solution to the problem of hallucinations in large language models (LLMs). Even with RAG, LLMs can still generate responses that hallucinate around the source material, leading to inaccuracies [\\[[1](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:0-5)\\]].\n",
      "\n",
      "2. **Misinterpretation and Contextual Understanding**: RAG systems may retrieve factually correct but misleading sources, leading to errors in interpretation. LLMs may struggle to consider the context of the retrieved information, potentially resulting in incorrect conclusions. Additionally, when faced with conflicting information, RAG models may find it challenging to determine the accuracy of sources and may merge details in a misleading way [\\[[2](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:2-2)\\]].\n",
      "\n",
      "3. **Verification of Source Credibility**: RAG systems do not inherently verify the credibility of the sources they retrieve from, which can lead to misleading or inaccurate responses. This can result in AI systems generating misinformation even when pulling from factually correct sources if they misinterpret the context of the information [\\[[3](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:2-2)\\]].\n",
      "\n",
      "4. **Training Data Aging and Source Knowledge**: LLMs may struggle with aging training data and the inability to distinguish specific sources of knowledge. This can lead to inaccuracies, particularly in time-sensitive queries, and challenges in maintaining the freshness and accuracy of information within the model [\\[[5](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:0-5)\\]].\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question 2: How does RAG improve factual accuracy compared to standard LLMs?\n",
      "--------------------------------------------------------------------------------\n",
      "Using existing collection: rag_documents\n",
      "Retrieved 5 chunks for query: 'How does RAG improve factual accuracy compared to standard LLMs?'\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 1 (Distance: 0.2856, Source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation):\n",
      "RAG is not a complete solution to the problem of hallucinations in LLMs. According to [_Ars Technica_](https://en.wikipedia.org/wiki/Ars_Technica \"Ars Technica\"), \"It is not a direct solution because the LLM can still hallucinate around the source material in its response.\"[\\[5\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:0-5)\n",
      "\n",
      "While RAG improves the accuracy of large language models (LLMs), it does not eliminate all challenges. One limitation is that while RAG reduces the need for frequent model retraining, it does not remove it entirely. Additionally, LLMs may struggle to recognize when they lack sufficient information to provide a reliable response. Without specific training, models may generate answers even when they should indicate uncertainty. According to [IBM](https://en.wikipedia.org/wiki/IBM \"IBM\"), this issue can arise when the model lacks the ability to assess its own knowledge limitations.[\\[1\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:1-1)\n",
      "\n",
      "RAG systems may retrieve factually correct but misleading sources, leading to errors in interpretation. In some cases, an LLM may extract statements from a source without considering its context, resulting in an incorrect conclusion. Additionally, when faced with conflicting information, RAG models may struggle to determine which source is accurate and may combine details from multiple sources, producing responses that merge outdated and updated information in a misleading way. According to the [_MIT Technology Review_,](https://en.wikipedia.org/wiki/MIT_Technology_Review \"MIT Technology Review\") these issues occur because RAG systems may misinterpret the data they retrieve.[\\[2\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:2-2)\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 2 (Distance: 0.4297, Source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation):\n",
      "RAG systems do not inherently verify the credibility of the sources they retrieve from, which can lead to misleading or inaccurate responses. AI systems can generate misinformation even when pulling from factually correct sources if they misinterpret the context. [_MIT Technology Review_](https://en.wikipedia.org/wiki/MIT_Technology_Review \"MIT Technology Review\") gives the example of an AI-generated response stating, \"The United States has had one Muslim president, Barack Hussein Obama.\" The model retrieved this from an academic book titled _Barack Hussein Obama: Americas First Muslim President?_ and misinterpreted the content, generating a false claim.[\\[2\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:2-2)\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) is a method that allows large language models (LLMs) to retrieve and incorporate additional information before generating responses. Unlike LLMs that rely solely on pre-existing [training data](https://en.wikipedia.org/wiki/Training_data \"Training data\"), RAG integrates newly available data at query time. _Ars Technica_ states, \"The beauty of RAG is that when new information becomes available, rather than having to retrain the model, all thats needed is to augment the models external knowledge base with the updated information.\"[\\[5\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:0-5)\n",
      "\n",
      "The _BBC_ describes \"prompt stuffing\" as a technique within RAG, in which relevant context is inserted into a prompt to guide the models response. This approach provides the LLM with key information early in the prompt, encouraging it to prioritize the supplied data over pre-existing training knowledge.[\\[6\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-6)\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 3 (Distance: 0.4561, Source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation):\n",
      "Beyond efficiency gains, RAG also allows LLMs to include source references in their responses, enabling users to verify information by reviewing cited documents or original sources. This can provide greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.[\\[4\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-AWS-4)\n",
      "\n",
      "RAG and LLM Limitations\n",
      "-----------------------\n",
      "\n",
      "\\[[edit](https://en.wikipedia.org/w/index.php?title=Retrieval-augmented_generation&action=edit&section=1 \"Edit section: RAG and LLM Limitations\")\\]\n",
      "\n",
      "In June 2024, _Ars Technica_ reported, \"But LLMs arent humans, of course. Their training data can age quickly, particularly in more time-sensitive queries. In addition, the LLM often cant distinguish specific sources of its knowledge, as all its training data is blended together into a kind of soup.\" In 2023, during its launch demonstration, [Googles Bard](https://en.wikipedia.org/wiki/Google_Bard \"Google Bard\") provided incorrect information about the [James Webb Space Telescope](https://en.wikipedia.org/wiki/James_Webb_Space_Telescope \"James Webb Space Telescope\"), an error that contributed to a $100 billion decline in [Alphabet](https://en.wikipedia.org/wiki/Alphabet_Inc. \"Alphabet Inc.\")s stock value.[\\[5\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:0-5)\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 4 (Distance: 0.4684, Source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation):\n",
      "RAG improves large language models (LLMs) by incorporating [information retrieval](https://en.wikipedia.org/wiki/Information_retrieval \"Information retrieval\") before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources.[\\[1\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:1-1) According to _[Ars](https://en.wikipedia.org/wiki/Ars_Technica \"Ars Technica\") [Technica](https://en.wikipedia.org/wiki/Ars_Technica \"Ars Technica\")_, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce [AI hallucinations](https://en.wikipedia.org/wiki/AI_hallucinations \"AI hallucinations\"), which have led to real-world issues like chatbots inventing policies or lawyers citing nonexistent legal cases.[\\[5\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:0-5)\n",
      "\n",
      "By dynamically retrieving information, RAG enables AI to provide more accurate responses without frequent retraining. According to [IBM](https://en.wikipedia.org/wiki/IBM \"IBM\"), \"RAG also reduces the need for users to continuously train the model on new data and update its parameters as circumstances evolve. In this way, RAG can lower the computational and financial costs of running LLM-powered [chatbots](https://en.wikipedia.org/wiki/Chatbot \"Chatbot\") in an enterprise setting.\"[\\[1\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:1-1)\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 5 (Distance: 0.5680, Source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation):\n",
      "Improvements to the basic process above can be applied at different stages in the RAG flow.\n",
      "\n",
      "These methods focus on the encoding of text as either dense or sparse vectors. [Sparse vectors](https://en.wikipedia.org/wiki/Sparse_vector \"Sparse vector\"), which encode the identity of a word, are typically [dictionary](https://en.wikipedia.org/wiki/Large_language_model#Tokenization \"Large language model\")\\-length and contain mostly zeros. [Dense vectors](https://en.wikipedia.org/wiki/Dense_matrix \"Dense matrix\"), which encode meaning, are more compact and contain fewer zeros. Various enhancements can improve the way similarities are calculated in the vector stores (databases).[\\[8\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:3-8)\n",
      "\n",
      "Answer:\n",
      "RAG (Retrieval-Augmented Generation) improves factual accuracy compared to standard LLMs by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely solely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources, allowing AI to provide more accurate responses without frequent retraining. This method helps reduce AI hallucinations and improves performance by blending the LLM process with a web search or document look-up process to help LLMs stick to the facts. RAG also reduces the need for users to continuously train the model on new data and update its parameters, lowering the computational and financial costs of running LLM-powered chatbots in an enterprise setting [4].\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question 3: What are some real-world applications of RAG technology?\n",
      "--------------------------------------------------------------------------------\n",
      "Using existing collection: rag_documents\n",
      "Retrieved 5 chunks for query: 'What are some real-world applications of RAG technology?'\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 1 (Distance: 0.4788, Source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation):\n",
      "RAG is not a complete solution to the problem of hallucinations in LLMs. According to [_Ars Technica_](https://en.wikipedia.org/wiki/Ars_Technica \"Ars Technica\"), \"It is not a direct solution because the LLM can still hallucinate around the source material in its response.\"[\\[5\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:0-5)\n",
      "\n",
      "While RAG improves the accuracy of large language models (LLMs), it does not eliminate all challenges. One limitation is that while RAG reduces the need for frequent model retraining, it does not remove it entirely. Additionally, LLMs may struggle to recognize when they lack sufficient information to provide a reliable response. Without specific training, models may generate answers even when they should indicate uncertainty. According to [IBM](https://en.wikipedia.org/wiki/IBM \"IBM\"), this issue can arise when the model lacks the ability to assess its own knowledge limitations.[\\[1\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:1-1)\n",
      "\n",
      "RAG systems may retrieve factually correct but misleading sources, leading to errors in interpretation. In some cases, an LLM may extract statements from a source without considering its context, resulting in an incorrect conclusion. Additionally, when faced with conflicting information, RAG models may struggle to determine which source is accurate and may combine details from multiple sources, producing responses that merge outdated and updated information in a misleading way. According to the [_MIT Technology Review_,](https://en.wikipedia.org/wiki/MIT_Technology_Review \"MIT Technology Review\") these issues occur because RAG systems may misinterpret the data they retrieve.[\\[2\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:2-2)\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 2 (Distance: 0.4922, Source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation):\n",
      "RAG improves large language models (LLMs) by incorporating [information retrieval](https://en.wikipedia.org/wiki/Information_retrieval \"Information retrieval\") before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources.[\\[1\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:1-1) According to _[Ars](https://en.wikipedia.org/wiki/Ars_Technica \"Ars Technica\") [Technica](https://en.wikipedia.org/wiki/Ars_Technica \"Ars Technica\")_, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce [AI hallucinations](https://en.wikipedia.org/wiki/AI_hallucinations \"AI hallucinations\"), which have led to real-world issues like chatbots inventing policies or lawyers citing nonexistent legal cases.[\\[5\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:0-5)\n",
      "\n",
      "By dynamically retrieving information, RAG enables AI to provide more accurate responses without frequent retraining. According to [IBM](https://en.wikipedia.org/wiki/IBM \"IBM\"), \"RAG also reduces the need for users to continuously train the model on new data and update its parameters as circumstances evolve. In this way, RAG can lower the computational and financial costs of running LLM-powered [chatbots](https://en.wikipedia.org/wiki/Chatbot \"Chatbot\") in an enterprise setting.\"[\\[1\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:1-1)\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 3 (Distance: 0.6130, Source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation):\n",
      "Retro language model for RAG. Each Retro block consists of Attention, Chunked Cross Attention, and Feed Forward layers. Black-lettered boxes show data being changed, and blue lettering shows the algorithm performing the changes.\n",
      "\n",
      "By redesigning the language model with the retriever in mind, a 25-time smaller network can get comparable perplexity as its much larger counterparts.[\\[16\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-borgeaud-16) Because it is trained from scratch, this method (Retro) incurs the high cost of training runs that the original RAG scheme avoided. The hypothesis is that by giving domain knowledge during training, Retro needs less focus on the domain and can devote its smaller weight resources only to language semantics. The redesigned language model is shown here.\n",
      "\n",
      "It has been reported that Retro is not reproducible, so modifications were made to make it so. The more reproducible version is called Retro++ and includes in-context RAG.[\\[17\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-wang2023a-17)\n",
      "\n",
      "Chunking involves various strategies for breaking up the data into vectors so the retriever can find details in it.\n",
      "\n",
      "[![Image 3](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0c/Rag-doc-styles.png/500px-Rag-doc-styles.png)](https://en.wikipedia.org/wiki/File:Rag-doc-styles.png)\n",
      "\n",
      "Different data styles have patterns that correct chunking can take advantage of.\n",
      "\n",
      "Three types of chunking strategies are:\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 4 (Distance: 0.6189, Source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation):\n",
      "Beyond efficiency gains, RAG also allows LLMs to include source references in their responses, enabling users to verify information by reviewing cited documents or original sources. This can provide greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.[\\[4\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-AWS-4)\n",
      "\n",
      "RAG and LLM Limitations\n",
      "-----------------------\n",
      "\n",
      "\\[[edit](https://en.wikipedia.org/w/index.php?title=Retrieval-augmented_generation&action=edit&section=1 \"Edit section: RAG and LLM Limitations\")\\]\n",
      "\n",
      "In June 2024, _Ars Technica_ reported, \"But LLMs arent humans, of course. Their training data can age quickly, particularly in more time-sensitive queries. In addition, the LLM often cant distinguish specific sources of its knowledge, as all its training data is blended together into a kind of soup.\" In 2023, during its launch demonstration, [Googles Bard](https://en.wikipedia.org/wiki/Google_Bard \"Google Bard\") provided incorrect information about the [James Webb Space Telescope](https://en.wikipedia.org/wiki/James_Webb_Space_Telescope \"James Webb Space Telescope\"), an error that contributed to a $100 billion decline in [Alphabet](https://en.wikipedia.org/wiki/Alphabet_Inc. \"Alphabet Inc.\")s stock value.[\\[5\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:0-5)\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 5 (Distance: 0.6209, Source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation):\n",
      "RAG systems do not inherently verify the credibility of the sources they retrieve from, which can lead to misleading or inaccurate responses. AI systems can generate misinformation even when pulling from factually correct sources if they misinterpret the context. [_MIT Technology Review_](https://en.wikipedia.org/wiki/MIT_Technology_Review \"MIT Technology Review\") gives the example of an AI-generated response stating, \"The United States has had one Muslim president, Barack Hussein Obama.\" The model retrieved this from an academic book titled _Barack Hussein Obama: Americas First Muslim President?_ and misinterpreted the content, generating a false claim.[\\[2\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:2-2)\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) is a method that allows large language models (LLMs) to retrieve and incorporate additional information before generating responses. Unlike LLMs that rely solely on pre-existing [training data](https://en.wikipedia.org/wiki/Training_data \"Training data\"), RAG integrates newly available data at query time. _Ars Technica_ states, \"The beauty of RAG is that when new information becomes available, rather than having to retrain the model, all thats needed is to augment the models external knowledge base with the updated information.\"[\\[5\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-:0-5)\n",
      "\n",
      "The _BBC_ describes \"prompt stuffing\" as a technique within RAG, in which relevant context is inserted into a prompt to guide the models response. This approach provides the LLM with key information early in the prompt, encouraging it to prioritize the supplied data over pre-existing training knowledge.[\\[6\\]](https://en.wikipedia.org/wiki/Retrieval-augmented_generation#cite_note-6)\n",
      "\n",
      "Answer:\n",
      "Some real-world applications of RAG technology include:\n",
      "\n",
      "1. **Improving Large Language Models (LLMs)**: RAG enhances the performance of LLMs by incorporating information retrieval before generating responses, helping these models stick to facts and reduce AI hallucinations that can lead to issues like chatbots inventing policies or citing nonexistent legal cases (Ars Technica) [1].\n",
      "\n",
      "2. **Reducing the Need for Frequent Retraining**: RAG enables AI to provide more accurate responses without requiring continuous retraining, lowering computational and financial costs in enterprise settings (IBM) [2].\n",
      "\n",
      "3. **Enhancing Transparency**: RAG allows LLMs to include source references in their responses, enabling users to verify information by reviewing cited documents or original sources, thus providing greater transparency (AWS) [4].\n",
      "\n",
      "4. **Dynamic Information Retrieval**: RAG dynamically retrieves information from databases, uploaded documents, or web sources, improving response accuracy without the need for constant model retraining (Ars Technica) [1].\n",
      "\n",
      "These applications highlight how RAG technology can be utilized to enhance the performance and reliability of large language models in various real-world scenarios.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define a list of questions to ask\n",
    "questions = [\n",
    "    \"What are the key challenges in implementing RAG systems?\",\n",
    "    \"How does RAG improve factual accuracy compared to standard LLMs?\",\n",
    "    \"What are some real-world applications of RAG technology?\"\n",
    "]\n",
    "\n",
    "# Ask each question and print the answers\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"\\nQuestion {i}: {question}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Retrieve relevant chunks\n",
    "    chunks, sources = retrieve_relevant_chunks(question)\n",
    "    \n",
    "    if chunks:\n",
    "        # Generate a response\n",
    "        response = generate_response(question, chunks, sources)\n",
    "        print(\"\\nAnswer:\")\n",
    "        print(response)\n",
    "    else:\n",
    "        print(\"No relevant information found to answer this question.\")\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b153a4b5",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've built a complete RAG system from scratch that can:\n",
    "1. Fetch and process documents from web URLs\n",
    "2. Chunk them intelligently using RecursiveTokenChunker\n",
    "3. Create embeddings using sentence-transformers\n",
    "4. Store them in a ChromaDB vector database\n",
    "5. Retrieve relevant information for queries\n",
    "6. Generate informed responses using OpenAI's API\n",
    "\n",
    "This basic implementation provides a solid foundation, but there are many ways to enhance your RAG system:\n",
    "\n",
    "1. **Add more documents**: Expand your knowledge base with more sources\n",
    "2. **Improve chunking**: Experiment with different chunking strategies and parameters\n",
    "3. **Try different embedding models**: Test models like OpenAI's embeddings or other sentence transformers\n",
    "4. **Enhance the retriever**: Implement re-ranking or hybrid search approaches\n",
    "5. **Refine prompt engineering**: Optimize your prompts for better responses\n",
    "6. **Add evaluation**: Measure the quality of your retrieval and generation\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Retrieval Augmented Generation (RAG): A comprehensive introduction](https://www.pinecone.io/learn/retrieval-augmented-generation/)\n",
    "- [ChromaDB Documentation](https://docs.trychroma.com/)\n",
    "- [Sentence-Transformers Documentation](https://www.sbert.net/)\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)\n",
    "- [Chunking Evaluation GitHub Repository](https://github.com/brandonstarxel/chunking_evaluation)\n",
    "- [Jina AI Reader](https://github.com/jina-ai/reader)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
